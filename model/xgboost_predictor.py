import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import optuna


class XGBoostPredictor:
    def __init__(self, symbol):
        self.symbol = symbol.replace("/", "_")
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.model = None
        self.best_params = None

    def optimize_hyperparameters(self, X, y, trials=50):
        def objective(trial):
            params = {
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            }

            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=False)
            model = xgb.XGBRegressor(**params, objective='reg:squarederror', eval_metric='rmse')
            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)
            preds = model.predict(X_valid)
            return mean_squared_error(y_valid, preds)

        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=trials)

        self.best_params = study.best_params
        print(f"üöÄ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {self.best_params}")
        return self.best_params

    def train(self, X, y, optimize=False, trials=50):
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        X_scaled = self.scaler_X.fit_transform(X)
        y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

        X_train, X_valid, y_train, y_valid = train_test_split(
            X_scaled, y_scaled, test_size=0.2, shuffle=False
        )

        if optimize:
            self.optimize_hyperparameters(X_train, y_train, trials=trials)
            model_params = self.best_params
        else:
            model_params = {
                'max_depth': 6,
                'learning_rate': 0.05,
                'n_estimators': 200,
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse'
            }

        self.model = xgb.XGBRegressor(**model_params)

        self.model.fit(
            X_train, y_train,
            eval_set=[(X_valid, y_valid)],
            verbose=False
        )

        preds = self.model.predict(X_valid)
        rmse = mean_squared_error(y_valid, preds) ** 0.5
        print(f"‚úÖ XGBoost-–º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞. RMSE –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {rmse:.4f}")

    def predict(self, X):
        X_scaled = self.scaler_X.transform(X)
        preds_scaled = self.model.predict(X_scaled)
        preds_real = self.scaler_y.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()
        return preds_real
