from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from torch import nn
from torch.optim.lr_scheduler import ReduceLROnPlateau
import os
import numpy as np

class PPOAgent:
    def __init__(self, env, model_dir='models', symbol='BTC_USDT', total_epochs=10):
        self.env = env
        self.model_dir = model_dir
        self.symbol = symbol
        self.total_epochs = total_epochs

        os.makedirs(self.model_dir, exist_ok=True)

        # –ü—Ä–æ—Å—Ç–∞—è –∏ –º–æ—â–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        policy_kwargs = dict(
            net_arch=[512, 256, 128],  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            activation_fn=nn.ReLU  # –ü—Ä–æ—Å—Ç–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        )

        self.model = PPO(
            "MlpPolicy",
            self.env,
            learning_rate=1e-4,  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –Ω–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            gamma=0.99,  # –£–º–µ—Ä–µ–Ω–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            gae_lambda=0.95,  # –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è GAE
            ent_coef=0.005,  # –£–º–µ—Ä–µ–Ω–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏–π–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –±–∞–ª–∞–Ω—Å–∞
            clip_range=0.1,  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            normalize_advantage=True,
            n_steps=2048,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            batch_size=64,  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            policy_kwargs=policy_kwargs
        )

        # –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ learning rate
        self.scheduler = ReduceLROnPlateau(self.model.policy.optimizer, 'min', patience=5, factor=0.5)

    def train(self, timesteps_per_epoch=10000, eval_freq=1000):
        eval_callback = EvalCallback(
            self.env,
            best_model_save_path=os.path.join(self.model_dir, 'best_model'),
            log_path=os.path.join(self.model_dir, 'eval_logs'),
            eval_freq=eval_freq,
            deterministic=True,
            verbose=1
        )

        checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=self.model_dir)

        for epoch in range(self.total_epochs):
            print(f"üöÄ –ó–∞–ø—É—Å–∫–∞—é —ç–ø–æ—Ö—É {epoch + 1}/{self.total_epochs}")
            self.model.learn(total_timesteps=timesteps_per_epoch, callback=[eval_callback, checkpoint_callback])

            # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–¥–Ω—é—é –Ω–∞–≥—Ä–∞–¥—É –∑–∞ —ç–ø–∏–∑–æ–¥—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç—Ä–∏–∫–∏
            avg_reward = np.mean([info['episode']['reward'] for info in self.model.ep_info_buffer])

            # –ü–æ–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫—É –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è learning rate
            self.scheduler.step(avg_reward)  # –ü–µ—Ä–µ–¥–∞–µ–º —Å—Ä–µ–¥–Ω—é—é –Ω–∞–≥—Ä–∞–¥—É –∫–∞–∫ –º–µ—Ç—Ä–∏–∫—É

            print(f"üìâ Learning rate –ø–æ—Å–ª–µ —ç–ø–æ—Ö–∏ {epoch + 1}: {self.scheduler.get_last_lr()[0]}")

    def save(self, filename="ppo_final"):
        path = os.path.join(self.model_dir, f'{self.symbol}_{filename}.zip')
        self.model.save(path=path)
        print(f"‚úÖ PPO-–º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {path}")

    def load(self, filepath):
        if os.path.exists(filepath):
            self.model = PPO.load(filepath, env=self.env)
            print(f"‚úÖ PPO-–º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")
        else:
            print(f"‚ö†Ô∏è PPO-–º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {filepath}")
