import os
import numpy as np
import torch
from PIL import Image
from torchvision import transforms

def precompute_model_predictions(env, lookahead=50):
    env.cached_lstm_preds = {}
    env.cached_cnn_preds = {}

    for future_step in range(env.current_step, env.current_step + lookahead):
        if future_step >= len(env.data):
            break

        # LSTM –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        env.cached_lstm_preds[future_step] = [
            model(torch.tensor(env.data[max(0, future_step - 20):future_step, :11],
                               dtype=torch.float32).unsqueeze(0)).item()
            for model in env.lstm_models.values()
        ]

        # CNN –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        cnn_preds_step = []
        transform = transforms.Compose([
            transforms.Resize((128, 128)),
            transforms.ToTensor()
        ])

        for tf, cnn_model in env.cnn_models.items():
            image_path = os.path.join("models", env.symbol.replace('/', '_'), "cnn",
                                      f"{env.symbol.replace('/', '_')}_{tf}", f"{future_step}.png")
            try:
                image = Image.open(image_path).convert('RGB')
            except FileNotFoundError:
                image = Image.new('RGB', (128, 128), color='black')

            cnn_input = transform(image).unsqueeze(0)
            with torch.no_grad():
                cnn_preds_step.append(cnn_model(cnn_input).item())
        env.cached_cnn_preds[future_step] = cnn_preds_step


def calculate_reward(env, action, current_price, atr, adx):
    reward = 0.0

    # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
    volatility_adj = np.clip(atr / current_price, 0.01, 0.1) if current_price != 0 else 0.01

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∏—Å–∫ –Ω–∞ —Å–¥–µ–ª–∫—É
    risk_percentage = 0.015  # –ë–∞–∑–æ–≤—ã–π —Ä–∏—Å–∫ 1.5% –æ—Ç –¥–µ–ø–æ–∑–∏—Ç–∞
    if adx > 30:  # –ï—Å–ª–∏ —Å–∏–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∏—Å–∫
        risk_percentage = 0.02  # –†–∏—Å–∫ 2% –æ—Ç –¥–µ–ø–æ–∑–∏—Ç–∞

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—É–º–º—É, –∫–æ—Ç–æ—Ä—É—é –∞–≥–µ–Ω—Ç –≥–æ—Ç–æ–≤ –ø–æ—Ç–µ—Ä—è—Ç—å
    loss_in_balance = env.balance * risk_percentage

    # –ó–∞–∫—Ä—ã—Ç–∏–µ –ø–æ–∑–∏—Ü–∏–∏
    if action == 0 and env.position and env.position_price > 0:
        profit = (current_price - env.position_price) if env.position == 'long' else (
            env.position_price - current_price)

        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –ø—Ä–∏–±—ã–ª–∏
        profit_pct = (profit / env.position_price) * 100
        reward = profit_pct * (1 + adx / 50) * volatility_adj  # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç—Ä–µ–Ω–¥ –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å

        # –ï—Å–ª–∏ –ø—Ä–∏–±—ã–ª—å–Ω–∞—è —Å–¥–µ–ª–∫–∞, —Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –±–æ–Ω—É—Å
        if profit > 0:
            reward += 0.1  # –ù–µ–±–æ–ª—å—à–æ–π –±–æ–Ω—É—Å –∑–∞ –ø—Ä–∏–±—ã–ª—å–Ω—É—é —Å–¥–µ–ª–∫—É

        # –®—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ –ø–æ—Ç–µ—Ä–∏
        if profit < 0:
            loss_pct = abs(profit) / env.position_price * 100
            reward -= loss_pct * 0.5  # –ß–µ–º –±–æ–ª—å—à–µ —É–±—ã—Ç–∫–∏, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ —à—Ç—Ä–∞—Ñ

        env.balance += profit - env.trading_fee
        env.pnl += profit
        env.position = env.position_price = env.position_open_step = None

    # –®—Ç—Ä–∞—Ñ –∑–∞ –±–µ–∑–¥–µ–π—Å—Ç–≤–∏–µ
    elif action == 0 and not env.position:
        reward = -0.05  # –®—Ç—Ä–∞—Ñ –∑–∞ –±–µ–∑–¥–µ–π—Å—Ç–≤–∏–µ

    # –û—Ç–∫—Ä—ã—Ç–∏–µ –ø–æ–∑–∏—Ü–∏–∏
    if action in [1, 2] and not env.position:
        env.position = 'long' if action == 1 else 'short'
        env.position_price = current_price
        env.position_open_step = env.current_step
        reward -= env.trading_fee / 10  # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—Ç–∫—Ä—ã—Ç–∏–µ –ø–æ–∑–∏—Ü–∏–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ—Ç–æ—Ä–≥–æ–≤–∫–∏

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ–¥–∏–Ω–∏—Ü (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã), –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–Ω–æ –∫—É–ø–∏—Ç—å/–ø—Ä–æ–¥–∞—Ç—å
        position_size = loss_in_balance / atr  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ATR –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ —Å–¥–µ–ª–∫–∏
        env.position_size = position_size  # –î–æ–±–∞–≤–ª—è–µ–º —ç—Ç–æ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞

    elif action in [1, 2] and env.position:
        reward -= 0.1  # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ –ø–æ–∑–∏—Ü–∏–∏

    # –£—Å–ª–æ–≤–∏—è SL –∏ TP —Å ADX –∏ ATR
    if env.position and env.position_price > 0:
        sl_price = (env.position_price - atr * env.sl_multiplier) if env.position == 'long' else (
                   env.position_price + atr * env.sl_multiplier)

        tp_multiplier = 4 if adx > 30 else 2 if adx < 15 else env.tp_multiplier
        tp_price = (env.position_price + atr * tp_multiplier) if env.position == 'long' else (
                   env.position_price - atr * tp_multiplier)

        # –£–±—ã—Ç–æ–∫ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ SL
        if (env.position == 'long' and current_price <= sl_price) or (
                env.position == 'short' and current_price >= sl_price):
            loss_pct = abs(current_price - env.position_price) / env.position_price * 100
            reward -= loss_pct * volatility_adj  # –ë–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ –∑–∞ —É–±—ã—Ç–æ—á–Ω—É—é —Å–¥–µ–ª–∫—É
            env.position = env.position_price = env.position_open_step = None

        # –ü—Ä–æ—Ñ–∏—Ç –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ TP
        elif (env.position == 'long' and current_price >= tp_price) or (
                env.position == 'short' and current_price <= tp_price):
            profit_pct = abs(current_price - env.position_price) / env.position_price * 100
            reward += profit_pct * volatility_adj  # –ë–æ–ª—å—à–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –ø—Ä–∏–±—ã–ª—å–Ω—É—é —Å–¥–µ–ª–∫—É
            reward += 0.5  # –ë–æ–Ω—É—Å –∑–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ TP
            env.position = env.position_price = env.position_open_step = None

    # –ë–æ–Ω—É—Å –∑–∞ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞ —Ç—Ä–µ–Ω–¥–æ–º (–µ—Å–ª–∏ ADX –≤—ã—Å–æ–∫–∏–π)
    if adx > 30:
        reward += 0.2  # –ë–æ–Ω—É—Å –∑–∞ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–Ω–¥—É

    # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–∏–µ –ø–æ–∑–∏—Ü–∏–∏ (–µ—Å–ª–∏ —É–¥–µ—Ä–∂–∏–≤–∞–µ–º –±–æ–ª—å—à–µ 50 —à–∞–≥–æ–≤)
    if env.position_open_step and (env.current_step - env.position_open_step) > 50:
        reward -= 0.1  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏

    # –®—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ —É–±—ã—Ç–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Ç–µ—Ä—è 25% –±–∞–ª–∞–Ω—Å–∞)
    if env.balance < env.initial_balance * 0.50:
        reward -= 20  # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ—Ç–µ—Ä—é –±–æ–ª–µ–µ 50% –±–∞–ª–∞–Ω—Å–∞

    # –ë–æ–Ω—É—Å –∑–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞–ø–∏—Ç–∞–ª–∞ (–∏–∑–±–µ–≥–∞—Ç—å –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏)
    balance_used_ratio = abs(env.balance - env.initial_balance) / env.initial_balance
    if balance_used_ratio < 0.25:  # –ï—Å–ª–∏ –∞–≥–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ–Ω–µ–µ 25% –±–∞–ª–∞–Ω—Å–∞
        reward += 0.2  # –ë–æ–Ω—É—Å –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–µ–∑–∫—É –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è
    reward = np.clip(reward, -500, 500)

    return reward



def get_observation(env):
    current_features = np.nan_to_num(env.data[env.current_step][1:12], nan=0.0)
    lstm_preds = env.cached_lstm_preds.get(env.current_step, [0.0] * len(env.lstm_models))
    cnn_preds = env.cached_cnn_preds.get(env.current_step, [0.0] * len(env.cnn_models))

    try:
        xgb_input_scaled = env.scaler_X.transform([current_features])
        xgb_pred_scaled = env.xgb_model.predict(xgb_input_scaled).item()
        xgb_pred = env.scaler_y.inverse_transform([[xgb_pred_scaled]])[0, 0]
    except Exception as e:
        print(f"üö® –û—à–∏–±–∫–∞ XGB-–ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ —à–∞–≥–µ {env.current_step}: {e}")
        xgb_pred = 0.0

    sentiment_score = (
        float(env.sentiment_scores[env.current_step])
        if env.sentiment_scores is not None and len(env.sentiment_scores) > env.current_step
        else 0.0
    )

    extra_features = [
        sentiment_score,
        float(env.position_open_step or 0),
        float(env.balance),
        float(env.pnl),
        float(abs(env.pnl / env.initial_balance) > 0.05),
        getattr(env, 'fear_greed_scaled', 0.5),
        env.trading_fee,
        env.sl_multiplier,
        env.tp_multiplier,
        xgb_pred
    ]

    obs = np.concatenate(
        [current_features, lstm_preds, cnn_preds, extra_features]
    ).astype(np.float32)

    # ‚úÖ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê–Ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    obs_normalized = (obs - obs.mean()) / (obs.std() + 1e-8)

    return obs_normalized
