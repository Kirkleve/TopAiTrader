import os

import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback
from trading.crypto_env import CryptoTradingEnv
from stable_baselines3.common.logger import configure

from trading.ppo_agent import PPOAgent


class PPOTrainer:
    def __init__(self, symbol, state_data, sentiment_scores, lstm_models,
                 cnn_model, cnn_scaler, xgb_model, xgb_scaler_X, xgb_scaler_y, model_manager, episodes=100000):
        self.symbol = symbol.replace('/', '_')
        self.state_data = state_data
        self.sentiment_scores = sentiment_scores
        self.lstm_models = lstm_models
        self.cnn_model = cnn_model
        self.cnn_scaler = cnn_scaler
        self.xgb_model = xgb_model
        self.xgb_scaler_X = xgb_scaler_X
        self.xgb_scaler_y = xgb_scaler_y
        self.model_manager = model_manager
        self.episodes = episodes

    def create_env(self):
        """–°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏."""
        return make_vec_env(lambda: CryptoTradingEnv(
            symbol=self.symbol,
            data=self.state_data,
            sentiment_scores=self.sentiment_scores,
            lstm_models=self.lstm_models,
            cnn_models=self.cnn_model,
            xgb_model=self.xgb_model,
            xgb_scaler_X=self.xgb_scaler_X,
            xgb_scaler_y=self.xgb_scaler_y,
        ), n_envs=1)

    def train(self, existing_model=None, eval_freq=10000, timesteps_per_epoch=5000, total_epochs=20):
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –∏ –æ—Ü–µ–Ω–æ—á–Ω–æ–π —Å—Ä–µ–¥—ã
        env = self.create_env()
        eval_env = self.create_env()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
        if not env.env_method('check_observations')[0]:
            raise ValueError("üö® –û—à–∏–±–∫–∞ –≤ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è—Ö, –æ–±—É—á–µ–Ω–∏–µ PPO –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ!")
        else:
            print("‚úÖ –ù–∞–±–ª—é–¥–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã. –ó–∞–ø—É—Å–∫–∞–µ–º PPO-–æ–±—É—á–µ–Ω–∏–µ!")

        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–±—ç–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=os.path.join(self.model_manager.model_dir, 'best_model'),
            log_path=os.path.join(self.model_manager.model_dir, 'eval_logs'),
            eval_freq=eval_freq,
            deterministic=True,
            verbose=1
        )

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (–±–µ–∑ tensorboard)
        log_path = os.path.join(self.model_manager.model_dir, 'ppo_logs')
        new_logger = configure(log_path, ["stdout", "csv"])

        # –°–æ–∑–¥–∞–Ω–∏–µ PPO –∞–≥–µ–Ω—Ç–∞
        agent = PPOAgent(
            env=env,
            model_dir=self.model_manager.model_dir,
            symbol=self.symbol,
            total_epochs=total_epochs
        )

        agent.model.set_logger(new_logger)

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        if existing_model:
            agent.model = PPO.load(existing_model, env=env)

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        for epoch in range(agent.total_epochs):
            print(f"üöÄ –≠–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è {epoch + 1}/{agent.total_epochs}")
            agent.model.learn(total_timesteps=timesteps_per_epoch * 2, callback=eval_callback)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –ø–µ—á–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ ep_info_buffer
            print("üîç –°–æ–¥–µ—Ä–∂–∏–º–æ–µ ep_info_buffer:", agent.model.ep_info_buffer)

            # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–¥–Ω—é—é –Ω–∞–≥—Ä–∞–¥—É –∑–∞ —ç–ø–∏–∑–æ–¥—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç—Ä–∏–∫–∏
            # –û–∂–∏–¥–∞–µ–º, —á—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–æ–π, –ø—Ä–æ–≤–µ—Ä–∏–º –∏ –∏–∑–≤–ª–µ—á–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É
            try:
                avg_reward = np.mean(
                    [info['reward'] for info in agent.model.ep_info_buffer])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'reward' –≤–º–µ—Å—Ç–æ 'episode'
            except KeyError:
                avg_reward = 0  # –í —Å–ª—É—á–∞–µ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –∫–ª—é—á–∞ 'reward', –∏—Å–ø–æ–ª—å–∑—É–µ–º 0

            # –ü–æ–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫—É –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è learning rate
            agent.scheduler.step(avg_reward)  # –ü–µ—Ä–µ–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫—É (—Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞)

            print(f"üìâ Learning rate –ø–æ—Å–ª–µ —ç–ø–æ—Ö–∏ {epoch + 1}: {agent.scheduler.get_last_lr()[0]}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
        agent.save()

        return agent.model, env

    def evaluate_agent(self, agent, env, episodes=50):
        total_rewards = []
        win_count = 0

        for episode in range(episodes):
            obs = env.reset()
            done = False
            total_reward = 0

            while not done:
                action, _ = agent.predict(obs, deterministic=True)
                obs, rewards, dones, infos = env.step(action)
                reward = rewards[0]
                done = dones[0]
                total_reward += reward

            total_rewards.append(total_reward)
            if total_reward > 0:
                win_count += 1

        avg_reward = np.mean(total_rewards)
        win_rate = (win_count / episodes) * 100

        print(f"\nüìä –û—Ü–µ–Ω–∫–∞ PPO-–∞–≥–µ–Ω—Ç–∞:")
        print(f"‚úÖ –°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å: {avg_reward:.2f}")
        print(f"‚úÖ –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —Å–¥–µ–ª–æ–∫: {win_rate:.2f}%")

        return agent, env
